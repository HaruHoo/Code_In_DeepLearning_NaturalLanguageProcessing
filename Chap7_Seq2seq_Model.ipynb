{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a78e84c3-d298-4be2-a962-af35b12a47aa",
   "metadata": {},
   "source": [
    "文本生成的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb64e21-5091-44d9-8f57-a6e207210106",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cppath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05481271-77db-4902-97c0-c08c5815a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Chap6 import RnnLM\n",
    "from Chap6 import BetterRnnLM\n",
    "from common.functions import softmax\n",
    "\n",
    "class RnnlmGen(RnnLM):\n",
    "    def generate(self,start_id,skip_ids=None,sample_size=100):   #start_size表示第一个单词的ID，sample_size表示要采样的单词数量\n",
    "        #skip_ids表示单词ID列表（被指定的单词将不被采样，用于排除PTB数据集中的<unk>、N等被预处理过的单词）\n",
    "        word_ids=[start_id]\n",
    "        \n",
    "        x=start_id\n",
    "        while len(word_ids)<sample_size:\n",
    "            x=np.array(x).reshape(1,1)  #mini-batch，所以即使是只输入1个单词ID的情况下，也要讲皮大小视为1，整理成1*1的数组\n",
    "            score=self.predict(x)  #输出各个单词的得分\n",
    "            p=softmax(score.flatten())  #正规化，得到想要的概率分布\n",
    "            \n",
    "            sampled=np.random.choice(len(p),size=1,p=p)  #根据概率分布选择下一个单词\n",
    "            if(skip_ids is None) or (sampled not in skip_ids):\n",
    "                x=sampled\n",
    "                word_ids.append(int(x))\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3826235c-e619-4dff-987d-3bac65031c2e",
   "metadata": {},
   "source": [
    "生成文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b16933c-dafd-448b-ae15-eacc2cf5546e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you named essentially sells curtailed marriage unique wages andreas investments sheets citing concrete gridlock demonstrate beverage chuck sitting xtra feed delayed portable foreseeable microprocessors battled rumored reducing legal raised stress parental mega-issues dr. beyond connolly solved outfit p&g fired competent admitting greatest pretrial trump riding dinkins syndicated feb. navigation overbuilt reverse newsletters equipment much environmentalists hacker eroded cancel tentatively suddenly sung termination elections complex cautious outlook ferdinand cans wellington appointments afterward opinions stayed reminded intimate cafeteria turnaround loath carson shelter towers new amazing clear borough stakes kangyo tremendous programming pesticides ranking bonuses deviation lawsuits permitted voter guards lloyd fat themselves.\n"
     ]
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "\n",
    "corpus,word_to_id,id_to_word=ptb.load_data('train')\n",
    "vocab_size=len(word_to_id)\n",
    "corpus_size=len(corpus)\n",
    "\n",
    "model=RnnlmGen()\n",
    "\n",
    "#设定start单词和skip单词\n",
    "start_word='you'\n",
    "skip_words=['N','<unk>','$']\n",
    "start_id=word_to_id[start_word]\n",
    "skip_ids=[word_to_id[w] for w in skip_words]\n",
    "\n",
    "#生成文本\n",
    "word_ids=model.generate(start_id,skip_ids)  #返回的是单词ID列表\n",
    "txt=' '.join([id_to_word[i] for i in word_ids])  #将单词ID列表转为句子   join：'分隔符'.join(列表) 连接单词\n",
    "txt=txt.replace(' <eos>', '.\\n')\n",
    "print(txt+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586556fd-0d21-4a1e-8fe8-560f804bbef4",
   "metadata": {},
   "source": [
    "使用更好的权重来进行文本生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b22b26a-1dd6-46fc-89b5-6953dfa70169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you classes tires easy gates always nashua insurance bipartisan growing avon embarrassing unspecified face municipalities acceptance merchandise portfolios trader diesel customers feed newspaper talks owner ag followed redemption expire links pricing guinea fluor adults speaking beginning equaling hair decent analysis ehrlich ai memories evenly offer minimal gillette constitution searching scams climb root apparel ted d.c. access cabernet owns vegas kemp screens norwegian riskier carl kobe jr. donaldson pearce thief revco concentrating across differences beam ltd. embarrassment mac caller asarco pick winners bart go fournier agricultural fibers fertilizer shell consideration measures advent getting disabilities prior casting omitted dinner shore one-time gillette.\n"
     ]
    }
   ],
   "source": [
    "from dataset import ptb\n",
    "\n",
    "corpus,word_to_id,id_to_word=ptb.load_data('train')\n",
    "vocab_size=len(word_to_id)\n",
    "corpus_size=len(corpus)\n",
    "\n",
    "model=RnnlmGen()\n",
    "model.load_params('Rnnlm.pkl')\n",
    "#设定start单词和skip单词\n",
    "start_word='you'\n",
    "skip_words=['N','<unk>','$']\n",
    "start_id=word_to_id[start_word]\n",
    "skip_ids=[word_to_id[w] for w in skip_words]\n",
    "\n",
    "#生成文本\n",
    "word_ids=model.generate(start_id,skip_ids)  #返回的是单词ID列表\n",
    "txt=' '.join([id_to_word[i] for i in word_ids])  #将单词ID列表转为句子   join：'分隔符'.join(列表) 连接单词\n",
    "txt=txt.replace(' <eos>', '.\\n')\n",
    "print(txt+'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3166d130-5d7e-4660-a463-b9c602347ee1",
   "metadata": {},
   "source": [
    "seq2seq处理加法数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26d876a-5ef8-4a17-90f2-5ec1ccbbc7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45000, 7) (45000, 5)\n",
      "(5000, 7) (5000, 5)\n",
      "[ 3  0  2  0  0 11  5]\n",
      "[ 6  0 11  7  5]\n",
      "71+118 \n",
      "_189 \n"
     ]
    }
   ],
   "source": [
    "from dataset import sequence\n",
    "(x_train,t_train),(x_test,t_test)=sequence.load_data('addition.txt',seed=1984)    #load_data读入指定文本文件，并将其转为字符ID，返回训练数据和测试数据\n",
    "char_to_id,id_to_char=sequence.get_vocab()  #get_vocab返回字符与ID的映射字典\n",
    "\n",
    "print(x_train.shape,t_train.shape)\n",
    "print(x_test.shape,t_test.shape)\n",
    "\n",
    "print(x_train[0])\n",
    "print(t_train[0])\n",
    "\n",
    "print(''.join([id_to_char[c] for c in x_train[0]]))\n",
    "print(''.join([id_to_char[c] for c in t_train[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a39350-15e5-448a-96a7-df34de74676b",
   "metadata": {},
   "source": [
    "seq2seq的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2544366-3fa7-4421-9758-27e2ca676059",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder:\n",
    "    def __init__(self,vocab_size,wordvec_size,hidden_size):\n",
    "        V,D,H=vocab_size,wordvec_size,hidden_size   #vocab_size:词汇量，字符的种类；wordvec_size是字符向量的维数，hidden_size是LSTM隐藏层状态的维数\n",
    "        rn=np.random.randn\n",
    "        \n",
    "        embed_W=(rn(V,D)/100).astype('f')\n",
    "        lstm_Wx=(rn(D,4*H)/np.sqrt(D)).astype('f')\n",
    "        lstm_Wh=(rn(H,4*H)/np.sqrt(H)).astype('f')\n",
    "        lstm_b=np.zeros(4*H).astype('f')\n",
    "        \n",
    "        self.embed=TimeEmbedding(embed_W)\n",
    "        self.lstm=TimeLSTM(lstm_Wx,lstm_Wh,lstm_b,stateful=False)  #无需保存LSTM的隐藏状态\n",
    "        \n",
    "        self.params=self.embed.params+self.lstm.params\n",
    "        self.grads=self.embed.grads+self.lstm.grads\n",
    "        self.hs=None\n",
    "    \n",
    "    def forward(self,xs):\n",
    "        xs=self.embed.forward(xs)\n",
    "        hs=self.lstm.forward(xs)  #取出TimeEmbedding层最后一个时刻的隐藏状态，作为编码器的forward输出\n",
    "        self.hs=hs\n",
    "        return hs[:,-1,:]\n",
    "    \n",
    "    def backward(self,dh):\n",
    "        dhs=np.zeros_like(self.hs)\n",
    "        dhs[:,-1,:]=dh\n",
    "        dout=self.lstm.backward(dhs)\n",
    "        dout=self.embed.backward(dout)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd36cd-389c-4266-8cf5-9537c70cd431",
   "metadata": {},
   "source": [
    "Decoder层的实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ab72d1-24a9-4f67-91e9-44ff384068db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder:\n",
    "    def __init__(self,vocab_size,wordvec_size,hidden_size):\n",
    "        V,D,H=vocab_size,wordvec_size,hidden_size\n",
    "        rn=np.random.randn\n",
    "        \n",
    "        embed_W=(rn(V,D)/100).astype('f')\n",
    "        lstm_Wx=(rn(D,4*H)/np.sqrt(D)).astype('f')\n",
    "        lstm_Wh=(rn(H,4*H)/np.sqrt(H)).astype('f')\n",
    "        lstm_b=np.zeros(4*H).astype('f')\n",
    "        affine_W=(rn(H,V)/np.sqrt(H)).astype('f')\n",
    "        affine_b=np.zeros(V).astype('f')\n",
    "        \n",
    "        self.embed=TimeEmbedding(embed_W)\n",
    "        self.lstm=TimeLSTM(lstm_Wx,lstm_Wh,lstm_b,stateful=True)\n",
    "        self.affine=TimeAffine(affine_W,affine_b)\n",
    "        \n",
    "        self.params,self.grads=[],[]\n",
    "        for layer in (self.embed,self.lstm,self.affine):\n",
    "            self.params+=layer.params\n",
    "            self.grads+=layer.grads\n",
    "            \n",
    "    def forward(self,xs,h): #Decoder学习时使用\n",
    "        self.lstm.set_state(h)\n",
    "        out=self.embed.forward(xs)\n",
    "        out=self.lstm.forward(out)\n",
    "        score=self.affine.forward(out)\n",
    "        return score\n",
    "        \n",
    "    def backward(self,dscore):\n",
    "        dout=self.affine.backward(dscore)\n",
    "        dout=self.lstm.backward(dout)\n",
    "        dout=self.embed.backward(dout)\n",
    "        dh=self.lstm.dh\n",
    "        return dh\n",
    "        \n",
    "    #Decoder生成时使用\n",
    "    def generate(self,h,start_id,sample_size):\n",
    "        sampled=[]\n",
    "        sample_id=start_id\n",
    "        self.lstm.set_state(h)\n",
    "        \n",
    "        for _ in range(sample_size):\n",
    "            x=np.array(sample_id).reshape((1,1))\n",
    "            out=self.embed.forward(x)\n",
    "            out=self.lstm.forward(out)\n",
    "            score=self.affine.forward(out)\n",
    "                \n",
    "            sample_id=np.argmax(score.flatten())\n",
    "            sampled.append(int(sample_id))\n",
    "                \n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af53048-3f49-45ec-ac59-f35ec5af57cc",
   "metadata": {},
   "source": [
    "seq2seq类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed08c08c-7c5a-477a-941c-7873e3f84235",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.base_model import BaseModel\n",
    "class Seq2seq(BaseModel):\n",
    "    def __init__(self,vocab_size,wordvec_size,hidden_size):\n",
    "        V,D,H=vocab_size,wordvec_size,hidden_size\n",
    "        self.encoder=Encoder(V,D,H)\n",
    "        self.decoder=Decoder(V,D,H)\n",
    "        self.softmax=TimeSoftmaxWithLoss()\n",
    "        \n",
    "        self.params=self.encoder.params+self.decoder.params\n",
    "        self.grads=self.encoder.grads+self.decoder.grads\n",
    "        \n",
    "    def forward(self,xs,ts):\n",
    "        decoder_xs,decoder_ts=ts[:,:-1],ts[:,1:]\n",
    "        \n",
    "        h=self.encoder.forward(xs)\n",
    "        score=self.decoder.forward(decoder_xs,h)\n",
    "        loss=self.softmax.forward(score,decoder_ts)\n",
    "        return loss\n",
    "    \n",
    "    def backward(self,dout=1):\n",
    "        dout=self.softmax.backward(dout)\n",
    "        dh=self.decoder.backward(dout)\n",
    "        dout=self.encoder.backward(dh)\n",
    "        return dout\n",
    "    \n",
    "    def generate(self,xs,start_id,sample_size):\n",
    "        h=self.encoder.forward(xs)\n",
    "        sampled=self.decoder.generate(h,start_id,sample_size)\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29320151-f733-47ff-bc98-26a1cda414c0",
   "metadata": {},
   "source": [
    "seq2seq的评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cbd0f48e-ef01-457b-aa12-8669169c35fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 1 |  iter 1 / 351 | time 9[s] | loss 2.56\n",
      "| epoch 1 |  iter 21 / 351 | time 198[s] | loss 2.53\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_509/1117144277.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0macc_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mcorrect_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/common/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, t, max_epoch, batch_size, max_grad, eval_interval)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;31m# 计算梯度，更新参数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 将共享的权重整合为1个\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmax_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_509/814654062.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mdh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_509/4287388243.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dscore)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mdh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/common/time_layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dhs)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mdxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/work/common/time_layers.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, dh_next, dc_next)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mdx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mdh_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdh_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdc_prev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset import sequence\n",
    "from common.optimizer import Adam\n",
    "from common.trainer import Trainer\n",
    "from common.util import eval_seq2seq\n",
    "\n",
    "#读入数据集\n",
    "(x_train,t_train),(x_test,t_test)=sequence.load_data('addition.txt')\n",
    "char_to_id,id_to_char=sequence.get_vocab()\n",
    "\n",
    "#设定超参数\n",
    "vocab_size=len(char_to_id)\n",
    "wordvec_size=16\n",
    "hidden_size=128\n",
    "batch_size=128\n",
    "max_epoch=25\n",
    "max_grad=5.0\n",
    "\n",
    "#生成模型/优化器/训练器\n",
    "model=Seq2seq(vocab_size,wordvec_size,hidden_size)\n",
    "optimizer=Adam()\n",
    "trainer=Trainer(model,optimizer)\n",
    "\n",
    "acc_list=[]\n",
    "for epoch in range(max_epoch):\n",
    "    trainer.fit(x_train,t_train,max_epoch=1,batch_size=batch_size,max_grad=max_grad)\n",
    "    \n",
    "    correct_num=0\n",
    "    for i in range(len(x_test)):\n",
    "        question,correct=x_test[[i]],t_test[[i]]\n",
    "        verbose=i<10\n",
    "        correct_num+=eval_seq2seq(model,question,correct,id_to_char,verbos)\n",
    "    acc=float(correct_num)/len(x_test)\n",
    "    acc_list.append(acc)\n",
    "    print('val acc %.3f%%' % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5196fbf-da71-46db-86cd-d6288164c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"The Change of Acc\")\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Acc')\n",
    "epochs=[i for i in range(0,26,1)]\n",
    "plt.plot(epochs,acc_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee4bf2b-f3f7-4eee-b35a-d821628bfc62",
   "metadata": {},
   "source": [
    "seq2seq的改进:Peeky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c40142bb-b1fa-429f-8adf-e1644a776c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from common.time_layers import *\n",
    "\n",
    "class PeekyDecoder:\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        rn = np.random.randn\n",
    "\n",
    "        embed_W = (rn(V, D) / 100).astype('f')\n",
    "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
    "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
    "        lstm_b = np.zeros(4 * H).astype('f')\n",
    "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
    "        affine_b = np.zeros(V).astype('f')\n",
    "\n",
    "        self.embed = TimeEmbedding(embed_W)\n",
    "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
    "        self.affine = TimeAffine(affine_W, affine_b)\n",
    "\n",
    "        self.params, self.grads = [], []\n",
    "        for layer in (self.embed, self.lstm, self.affine):\n",
    "            self.params += layer.params\n",
    "            self.grads += layer.grads\n",
    "        self.cache = None\n",
    "\n",
    "    def forward(self, xs, h):\n",
    "        N, T = xs.shape\n",
    "        N, H = h.shape\n",
    "\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        out = self.embed.forward(xs)\n",
    "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "\n",
    "        out = self.lstm.forward(out)\n",
    "        out = np.concatenate((hs, out), axis=2)\n",
    "\n",
    "        score = self.affine.forward(out)\n",
    "        self.cache = H\n",
    "        return score\n",
    "\n",
    "    def backward(self, dscore):\n",
    "        H = self.cache\n",
    "\n",
    "        dout = self.affine.backward(dscore)\n",
    "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
    "        dout = self.lstm.backward(dout)\n",
    "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
    "        self.embed.backward(dembed)\n",
    "\n",
    "        dhs = dhs0 + dhs1\n",
    "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
    "        return dh\n",
    "\n",
    "    def generate(self, h, start_id, sample_size):\n",
    "        sampled = []\n",
    "        char_id = start_id\n",
    "        self.lstm.set_state(h)\n",
    "\n",
    "        H = h.shape[1]\n",
    "        peeky_h = h.reshape(1, 1, H)\n",
    "        for _ in range(sample_size):\n",
    "            x = np.array([char_id]).reshape((1, 1))\n",
    "            out = self.embed.forward(x)\n",
    "\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            out = self.lstm.forward(out)\n",
    "            out = np.concatenate((peeky_h, out), axis=2)\n",
    "            score = self.affine.forward(out)\n",
    "\n",
    "            char_id = np.argmax(score.flatten())\n",
    "            sampled.append(char_id)\n",
    "\n",
    "        return sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ae4a0bf4-a215-4572-95d8-758c3b57fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PeekySeq2seq(Seq2seq):\n",
    "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
    "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
    "        self.encoder = Encoder(V, D, H)\n",
    "        self.decoder = PeekyDecoder(V, D, H)\n",
    "        self.softmax = TimeSoftmaxWithLoss()\n",
    "\n",
    "        self.params = self.encoder.params + self.decoder.params\n",
    "        self.grads = self.encoder.grads + self.decoder.grads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.8",
   "language": "python",
   "name": "pytorch-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
